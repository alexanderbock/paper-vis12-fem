\documentclass[journal]{vgtc}                % final (journal style)
%\documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)
%\documentclass[electronic,journal]{vgtc}     % electronic version, journal

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Please note that the use of figures other than the optional teaser is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures other than the optional teaser on the
%% first page will be refused.

%% These three lines bring in essential packages: ``mathptmx'' for Type 1
%% typefaces, ``graphicx'' for inclusion of EPS figures. and ``times''
%% for proper handling of the times font family.

\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{times}

\usepackage{flushend}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{vector}
\usepackage{algorithmic}
\usepackage{algorithm}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\setlength\fboxsep{0pt}

\usepackage{color}
\newcommand{\warn}[1]{{\color{red} [#1]}}
\newcommand{\todo}[1]{\textbf{\textcolor{blue}{[TODO: {#1}]}}}

%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% This turns references into clickable hyperlinks.
\usepackage[bookmarks,backref=false,linkcolor=black]{hyperref} %,colorlinks
\hypersetup{
  pdfauthor = {},
  pdftitle = {},
  pdfsubject = {},
  pdfkeywords = {},
  colorlinks=true,
  linkcolor= black,
  citecolor= black,
  pageanchor=true,
  urlcolor = black,
  plainpages = false,
  linktocpage
}

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{229}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Algorithm/Technique}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{Coherency-based Curve Compression\\ for High-Order Finite Element Model Visualization}

%% This is how authors are specified in the journal style

%% indicate IEEE Member or Student Member in form indicated below
\author{Alexander Bock, Erik Sund\'en, Bingchen Liu,\\Burkhard W{\"u}nsche, \textit{Member, IEEE}, and Timo Ropinski, \textit{Member, IEEE}}
\authorfooter{
%% insert punctuation at end of each item
\item
 Alexander Bock,  Erik Sund\'en and Timo Ropinski are with the Scientific Visualization Group, Link\"oping University.
\item
 Bingchen Liu and Burkhard W{\"u}nsche are with the Bioengineering Institute Engineering Science, University of Auckland.
}

%other entries to be set up for journal
%\shortauthortitle{Biv \MakeLowercase{\textit{et al.}}: Global Illumination for Fun and Profit}
%\shortauthortitle{Firstauthor} \MakeLowercase{\textit{et al.}}: Paper Title


%% Abstract section.
\abstract{Finite element (FE) models are frequently used in engineering and life sciences to represent the results of time-consuming simulations. In contrast to the regular grid structure facilitated by volumetric data sets, as used in medicine or geosciences, FE models are defined over a non-uniform grid. Elements can have curved faces and their interior can be defined through high-order basis functions, which pose additional challenges when visualizing these models. During ray-casting, the uniformly distributed sample points along each viewing ray must be transformed into the material space defined within each element. However, the computational complexity of this transformation makes a straightforward approach inadequate for interactive data exploration. In this paper, we introduce a novel coherence-based method which supports the interactive exploration of FE models by decoupling the expensive world-to-material space transformation from the rendering stage, thereby allowing it to be performed within a precomputation stage. Thus, it becomes possible to visually analyze high-order FE models at interactive frame rates, even when they are time-varying or consist of multiple modalities. Within this paper, we provide the necessary background about the FE data, describe our decoupling method, and introduce our interactive rendering algorithm. Furthermore, we provide visual results and analyze the error introduced by the presented approach.} % end of abstract

%% Keywords that describe your work. Will show as 'Index Terms' in journal
%% please capitalize first letter and insert punctuation after last keyword
\keywords{Finite element visualization, GPU-based ray-casting.}

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CCScat'' command takes four arguments.

\CCScatlist{ % not used in journal version
 \CCScat{I.3.7}{Computer Graphics}{Three-Dimensional Graphics and Realism}{Color, shading, shadowing, and texture}
}

%% Uncomment below to include a teaser figure.
\teaser{
	\centering 
  \subfigure{\includegraphics[width=0.22\linewidth]{figures/Heart1New1_T1}}
  \subfigure{\includegraphics[width=0.22\linewidth]{figures/Heart1New2_T2}}
  \subfigure{\includegraphics[width=0.22\linewidth]{figures/Heart4New1_T3}}
  \subfigure{\includegraphics[width=0.22\linewidth]{figures/HeartCombined}}
  \caption{Application of our visualization approach to a multi-parametric FE model of the left ventricle of a human heart. The images show volumetric renderings of different strain directions defined over the FE model: radial strain, circumferential strain, longitudinal strain as well as the three former combined ({\it left to right}). The data can be explored interactively by changing the transfer function and all other relevant rendering parameters.}
  \label{fig:teaser}
}

%% Uncomment below to disable the manuscript note
%\renewcommand{\manuscriptnotetxt}{}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}\label{sec:introduction}

\maketitle

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/workflow-new.pdf}
    \caption{\textbf{Workflow.} To enable interactive ray-casting of curvilinear FE models, we exploit a multi-stage preprocessing approach. In the first step, we compute representative proxy rays, which are in the second step represented as splines and transformed into a common coordinate system. To reduce the data amount for the subsequent rendering stage, we apply a rotation invariant spline clustering in the third step. The resulting data, is then passed to the GPU-based rendering stage.}
    \label{fig:workflow}
\end{figure*}


The finite element (FE) method is a computational technique frequently applied in science, medicine, and engineering to solve partial differential and integral equations~\cite{Young00}. The technique enables the discretization of complicated domains into arbitrarily shaped elements, in which material properties can be defined using nodal parameters together with potentially high-order interpolation functions. Thus, it becomes possible to incorporate material properties at every point rather than a limited number of sample points. Applications range from simulations of physical processes, e.\,g.,~disease processes or turbulent flow simulations, over derivation of new entities from measured data, e.\,g.,~computation of material strain from displacement data, to analyses and comparisons of data defined over objects with similar topology. All of these applications involve large amounts of multi-dimensional and multi-parametric data defined over arbitrarily shaped grid structures. A prominent application example from bioengineering, where FE visualization is essential, is the strain analysis of the human heart. An understanding of the full 3D strain tensor, represented by a symmetric matrix at each point, is necessary to identify unusual behavior of the heart muscle's deformation and to detect common patterns indicating developing diseases. Since there is little knowledge available about the exact behavior, interactive exploration plays an important role in the quest for new discoveries. Throughout this paper we consider visualization techniques as interactive, when they enable a rapid response to human input. In our case, it should be possible to interactively change all relevant visualization parameters, e.\,g., parameter mapping and view orientation, such that the latency does not hinder the volumetric exploration process. While interactive surface-based visualizations of FE models are well established, volumetric visualization techniques supporting the interactive analysis of the entire 3D structure are rare~\cite{Wihelms90}. This is largely due to the fact, that accessing the materials defined over high-order basis functions, which needs to be done for each sample in a volumetric visualization approach, involves high computational complexity. To avoid this computational complexity during the rendering stage of the visualization pipeline, FE models are often sampled to regular grids and rendered using standard volume rendering. However, this introduces errors, removes the relation between material geometry and material properties, and furthermore fixes the resolution at which the data can be viewed. On top of this, resampling results in an extensive increase of the memory requirements, since it has to be performed for each parameter contained in a multi-parametric data set.

In this paper, we introduce an interactive volumetric ray-casting algorithm for FE models which exploits ray coherency. When ray-casting FE models, viewing rays are straight lines in world space, but bent to curves in material space. Consequently, each sample point on a straight viewing ray must be transformed during rendering onto the corresponding curve in material space, before the FE data can be fetched. As this is a computationally complex transformation, we exploit ray coherency in order to precompute a view-independent proxy representation of the curved rays, which can be accessed efficiently during rendering. The workflow of our approach is shown in Figure~\ref{fig:workflow}. In the first step, we compute a representative subset of curves in material space, which are represented by splines. This subset is then reduced through clustering within the second step, to obtain a lower number of curves, to which we refer as \emph{proxy rays}. Assuming that the proxy rays sufficiently represent the coherent curves of a FE model, we can access them in the third step, the rendering step, where we can omit any coordinate system transformations. This allows us to enable FE model visualization on modern graphics processing units (GPUs) at interactive frame rates. Thus, full volumetric and interactive exploration of potentially multi-parametric FE models, defined by high-order basis functions, on a single GPU is supported. As we achieve this interactivity by exploiting preprocessing algorithms and data structures to be accessed during rendering, the costs of our approach are increased precomputation times as well as memory consumption, which are discussed in more detail within Section~\ref{subsec:performance}. While our current implementation is constraint to hexahedral FE models, we will discuss the implications for other cell types within Section~\ref{sec:conclusions}.

The remainder of the paper is structured as follows. In the next section, we will describe previous work related to FE visualization. Section~\ref{sec:theory} explains the theory behind the FE data to be rendered, and Section~\ref{sec:preprocessing} describes the concepts underlying the data processing as well as the data layouts of the visualization approach presented. Section~\ref{sec:rendering} discusses the actual GPU-based ray-casting, and in Section~\ref{sec:results} we describe the results achieved, before concluding the paper in Section~\ref{sec:conclusions}.



\section{Related Work}\label{sec:relatedwork}
The FE method has several visualization related applications, such as the fitting of data to complex geometries, e.\,g., for strain computation from tagged MRI images~\cite{Young00}, surgical simulation~\cite{Berkley04} and breast image registration~\cite{lee10deformation}. In such applications, the usage of curvilinear elements is popular, since they enable an appropriate modeling of these complex shapes by minimizing the necessary number of elements~\cite{gelberg90visTechGrid,smith05ventricularmechanics}.

However, interactive high-quality rendering, incorporating the interior of the elements, is a demanding task. Often, the elements are resampled to lie on a common regular uniform grid~\cite{Wihelms90}. While this enables a direct visualization using standard volume rendering algorithms, it results in sampling-based errors as well as a loss of the easy access to element boundaries. Furthermore, resampling of multi-variate data sets would result in a vast data increase, which would need to be accessible during rendering. Moreover, the loss of the material coordinate system information, which goes hand in hand with resampling, is a major drawback in many engineering applications where the interpretation of a field value only makes sense relative to the underlying material geometry, e.\,g., flow direction relative to the arterial wall or myocardial strain relative to the circumferential and radial direction of the ventricular muscle. As a consequence, many researchers have investigated rendering techniques that enable a more direct FE visualization by maintaining the relationship between the world and the material coordinate system.

W{\"u}nsche introduced an isosurface-based approach for the visualization of biomedical tensor field data~\cite{wunsche03femvis}, which has been applied for strain analysis. Another technique for rendering isosurfaces was presented by Nelson~et~al. in which they regarded spectral high-order finite element meshes and introduced the compelling idea of an error budget~\cite{nelsonhp06}. Schroeder~et~at. realized a framework for isosurface rendering by tessellating the basis functions into simpler forms~\cite{schroeder06femtess}. Meyer~et~al. chose an approach based on particle systems to render isosurfaces efficiently~\cite{Meyer06}.

An alternative to the isosurface-based method was presented by Brasher~et~al. and Nelson~et.al., in which cut planes through the high-order finite element data are evaluated. Rose and Ertl proposed a texture-based mesh reduction approach in order to render surface representations of large FE models at interactive frame rates~\cite{rose03femvis}. Other authors propose algorithms which exploit a conversion into a tetrahedral mesh representation~\cite{Reed95IncrementalSlicing, marmitt05femtess, Georgii06ageneric}, which in comparison to regular grid sampling allow for adaptive representations.

To improve rendering quality, ray-casting approaches have been proposed. Garrity~\cite{garrity90RaytracingIrregular} was the first to introduce ray-casting for rendering of unstructured meshes. The technique is suitable for FE models, as it tracks numerous entry and exit points of a ray from the cells, which are stored in a hierarchical spatial data structure. Hong and Kaufman ~\cite{hong98curvilinear} incorporate the projection approach into ray-casting to speed up the ray traversal and interpolation. They later improved their method by employing a projection of the elements' faces onto the image plane, which allows them to compute ray intersection points for the ray traversal more efficiently~\cite{hong99curvilinear}. A similar strategy for finding intersection points is used by Farias et al., who apply a space sweeping approach~\cite{farias00zsweep}. Grimm et al. perform the intersection point computation on the GPU~\cite{grimm04curvilinear}. Their hybrid CPU/GPU approach performs a layer peeling on the GPU, while the actual ray-traversal is performed on the CPU. Martin et al ~\cite{MartinCurvilinearPacificVIS08} utilize a special representation of curvilinear space for transformation between physical space and computational space, using a linear approximation.

Moreland and Angel presented a method based on a partial pre-integration of the volume rendering integral and achieved interactive frame rates when rendering linear tetrahedral meshes~\cite{morelandVolRen04}. Wiley~et~al. demonstrated a technique to perform ray-casting on quadratic curved elements, but did so without regard to interactivity. This method could achieve interactive results on current machines but is limited to quadratic elements, which simplifies the inversion of the necessary coordinate transform~\cite{wileyraycasting04}.

More recently, {\"U}ffinger et al. have presented a distributed visualization approach that allows volumetric visualization of models to be defined through high-order polynomials~\cite{uffinger10femraycasting}. As ours, their approach is also based on ray-casting. However, in contrast to our approach, they circumvent computation of the world-to-material space transformation, by transforming the FE solution into a Cartesian reference space with barycentric coordinates. The FE interpolation is then replaced by a compact monomial representation, which the ray-casting kernel can use during sampling in physical space.

%\todo{Reformulate this paragraph: more structure and integrate new references.} Other authors propose algorithms which exploit a conversion into a tetrahedral mesh representation~\cite{Reed95IncrementalSlicing, marmitt05femtess, Georgii06ageneric}, which in comparison to regular grid sampling allow for adaptive representations. Alternatively, splatting-based approaches have been applied~\cite{mao95femsplatting}, and view-independent point sampling strategies have been exploited to deal with high-order data~\cite{zhou06pointbased}.


\section{Curvilinear Finite Element Models}\label{sec:theory}

While the concepts presented are in theory applicable to any FE model, they unfold their full potential when applied to curvilinear FE models, where the material space is defined through high-order basis functions. The curvilinear element examples presented in this paper employ cubic interpolation functions, whereby a tricubic element uses three degree 3 interpolation function. In several domain areas, as for instance in bioengineering~\cite{lee10deformation,wuensche03deformation}, the number of FE elements is relatively low and high-order interpolation functions are used.

FE models are defined through two coordinate systems. The \emph{material} or \emph{$\xi$ coordinate system} defines elements using simple geometric shapes, e.g. tetrahedra, prisms, pyramids, or unit cubes, which are decoupled from the actual geometry. The examples in this work are constructed based on unit cubes. In this case, the material coordinate system is cartesian with $\xi_1, \xi_2, \xi_3 \in [0,1]$ as basis for a right-handed coordinate system. The seconds coordinate system is the \emph{world coordinate system} which provides the position and orientation of the elements in space by defining a set of nodal points $x^{(e_j)}$ for each element $e_j$ with $\xi_i(1)=x^{(e_j)}_{2,i},\, i=1,2,3$ such that any point in the domain has a world coordinate $x$ with an associated $\xi$ coordinate. This association is provided by a set of mapping functions $\phi_i(\xi)$. The dependent variables are almost always expressed in $\xi$ coordinates, since $\xi$ coordinates are independent of the element geometry, and thus support the analysis of multi-parametric data and the comparison of different FE models. Thus, the dependent variables and the FE interpolation functions are defined in $\xi$ space, while the nodal points are defined in world space. This combination allows us to exploit the interpolation functions in order to compute variable values for all world space coordinates lying inside the model.

%First, the material or $\xi$ coordinate system, and second, the world coordinate system. The $\xi$ coordinate system is usually given in Cartesian coordinates and elements are represented using simple geometric shapes such as (unit) cubes or tetrahedra. The world coordinate system specifies the position and orientation of the elements in space by defining a set of nodal points $x^{(e_j)}$, where $e_j$ is an element. The dependent variables are almost always expressed in $\xi$ coordinates, since $\xi$ coordinates are independent of the element geometry, and thus support the analysis of multi-parametric data and the comparison of different FE models. Thus, the dependent variables and the FE interpolation functions are defined in $\xi$ space, while the nodal points are defined in world space. This combination allows us to exploit the interpolation functions in order to compute variable values for all world space coordinates lying inside the model. More formally, the relation between $\xi$ and world coordinates can be expressed as follows. If we assume that each element $e_j$ is defined over the $[0,1]^3$ interval in $\xi$ space, i.\,e., $\xi_i(0)=x^{(e_j)}_{1,i}$ and $\xi_i(1)=x^{(e_j)}_{2,i},\, i=1,2,3$, any point in the domain has a world coordinate $x$ with an associated $\xi$ coordinate. If the same basis functions $\phi_i$ are used for the geometry and the unknown variables, the mapping $x(\xi)$ is called isoparametric. However, in many practical applications, the interior of an elements is defined through high-order interpolation functions.

\begin{figure}[t]
    \centering
    \subfigure[world space]{\includegraphics[width=0.35\linewidth, height=0.3\linewidth]{figures/splines_world_space.pdf}}
    \subfigure[$\xi$ space]{\includegraphics[width=0.30\linewidth, height=0.3\linewidth]{figures/splines_xi_space.pdf}}
    \subfigure[world $\mapsto \xi$]{\includegraphics[height=0.3\linewidth]{figures/viewingrays.pdf}}
    \caption{When transforming straight rays given in world space {\it (a)} into $\xi$ space {\it (b)}, these rays are bent and need to be represented as curves. View rays which are parallel and similar in world space {\it (c,left)} will be projected to similar curves in $\xi$ space \it{(c,right)}.}
    \label{fig:raycoords}
\end{figure}

When visualizing such models, world coordinates are usually more convenient to use, as they define the position and orientation of the elements. However, as soon as the dependent variables have to be accessed, the $\xi$ coordinates need also be taken into account, as the dependent variables can only be accessed through transformation into $\xi$ space. Thus, the world coordinate system can be used for finding ray intersection points with the elements, while the actual ray traversal is performed in $\xi$ space. As can be seen in Figure~\ref{fig:raycoords}, this transformation affects the geometry of the viewing rays, which can end up as curves in $\xi$ space. While the transformation from $\xi$ space to world space can be computed fairly easy as $x(\xi)=\sum_{i=1}^n x_i \phi_i(\xi)$, for $n$ nodal coordinates $x_i$, and basis functions $\phi_i$, the inverse transformation required during ray-casting is computationally much more demanding. Numerical methods, e.\,g., the multi-dimensional Newton method~\cite{Press92}, which are often applied to perform this transformation, involve high computational complexity and thus are not feasible in an interactive context. Therefore, alternative approaches are necessary to avoid the computational complexity of the world to $\xi$ space transformation in the rendering stage of the visualization pipeline.

\section{Coherency-based Curve Compression}\label{sec:preprocessing}
%\begin{figure}[t]
%    \centering
%    \includegraphics[height=0.4\linewidth]{figures/viewingrays.pdf}
%    \caption{Rays which are adjacent in world space ({\it left}) have similar geometric properties in $\xi$ space ({\it right}). Color coding is applied for better tracing of ray-segments as they intersect several elements in $\xi$ space.}
%    \label{fig:coherency}
%\end{figure}

In this section, we present the basis for a novel ray-casting based algorithm for the interactive exploration of curvilinear FE models.

The \emph{straightforward approach} for ray-casting FE models would be to march along a ray in world space and transform each world space sample coordinate $x$ into the corresponding material coordinate $\xi$ before accessing the dependent variables. As discussed above and shown in Table~\ref{tab:performance}, this approach is computationally very demanding and therefore cannot be performed at interactive frame rates. By using our approach, we are able to shift this originally view-dependent computation from the rendering stage into the data processing stage of the visualization pipeline. Thus, we can reduce the computation complexity necessary during rendering and achieve interactive frame rates when volume rendering curvilinear FE models.

To perform the desired shift from the rendering stage into the data processing stage, we exploit the following key observations about the degree of coherency of curvilinear FE models. FE method grids are designed to minimize approximation errors in the solution and to ensure convergence. In particular the Jacobian of the world-to-material coordinate mapping must be positive definite, and the grid resolution needs to be high enough that variations of the unknown variable can be approximated adequately with the underlying FE basis functions~\cite{knupp07meshquality}. This implicit smoothness constraint is exploited in our technique, since it also limits the shape variations of neighboring rays in world coordinates mapped into material space, i.\,e., we can assume some degree of intra-element coherence. Because of the positive definiteness of the Jacobian we can employ the Newton method or similar solvers to compute the inverse of the material-to-world coordinate mapping at a point.

Many FE applications require also some degree of inter-element compatibility, e.\,g., for the column problem involving axial deformations. Furthermore, many real-world objects have some degree of symmetry and self-similarity. These characteristics result in a FE mesh with clusters of similar elements and hence some degree of inter-element coherence. Figure~\ref{fig:raycoords}~(c) illustrates this coherency, as it shows the shape of rays in $\xi$ space ({\it right}), which adjacently traverse world space ({\it left}). The element similarity is further demonstrated in the examples shown in Section~\ref{sec:results}.

\noindent In addition, coherence is supported as most models in practical applications need {\it well-behaved} elements (low distortion and an aspect ratio close to one), in order to result in a stable numerical simulation. Consequently, we exploit inter- and intra-element coherence to reduce the computational complexity involved in ray-casting of FE models. The approach presented uses precomputation, which is performed within two preprocessing steps as shown in Figure~\ref{fig:workflow}. In the first step, we compute a high number of proxy rays in $\xi$ space. These proxy rays act as a view- and resolution-independent spline representations of the actual viewing rays, which are cast through the material space during rendering. In the second step, we exploit inter- and intra-element coherence to reduce the vast number of proxy rays to a representative subset, which is the used during GPU-based ray-casting in the third step. By exploiting the precomputed data, FE models can be explored interactively by changing the transfer function and other rendering parameters. The following two subsections describe the two preprocessing steps, while the GPU-based ray-casting is discussed in Section~\ref{sec:rendering}.

\begin{figure}[b]
	\centering
	\includegraphics[width=0.75\linewidth]{figures/splinetransformation_new}
	\caption{To achieve a higher degree of similarity during the curve compression, we exploit the orientation invariance of the proxy rays and transform them into a common coordinate frame, where the principal curve axis coincidences with the $z$-axis. Furthermore, we exploit scale invariance to align all exit points.}
	\label{fig:proxyrayalignment}
\end{figure}


\subsection{Proxy Ray Generation}\label{subsec:proxyraygeneration}

To be able to compute a complete set $R$ of proxy rays, it is essential that it covers all parts of the FE model. This means, that both the world and the $\xi$ space are densely populated with straight and curved rays, respectively. However, since the view direction used during rendering affects the ray traversal direction, it is not sufficient that all positions within an element are close to a proxy ray. They must also be close to proxy rays having different directions, which represent different viewing directions affecting the ray traversal. As the interpolation functions in $\xi$ space can be different for each element within one FE model, elements are treated individually during preprocessing and rendering. Finding the desired dense and omni directional proxy ray population for an element can be performed in two ways. It can be either done adaptively by analyzing the actual ray distribution within each element to find areas of high importance and create more rays for these areas, or in a uniform way over each element. As our algorithm is based on the inter- and intra-element coherence , the latter distribution is sufficient since we assume a continuity between neighboring rays without high frequency changes.

To obtain such a uniform proxy ray distribution, we exploit the intra-element coherence, which results in a high similarity between rays in $\xi$ space which enter and exit an element through adjacent coordinates. Therefore, we compute a dense and uniform distribution $P$ of potential entry and exit points in $\xi$ coordinates on the surface of each element. While these points should be distributed over the entire surface of an element, the requirements with respect to the uniformity of $P$ are rather loose due to the intra-element coherence. We distribute the potential entry and exit points by performing an equidistant subdivision of all faces of an element. In this step, the subdivision parameter $s_f$ controls the number of points equidistantly positioned along the $u$ and the $v$ coordinates of a face. Thus, the density of $P$ can be directly controlled by changing the subdivision parameter $s_f$. To reduce the number of initially generated proxy rays, it would also be an option to vary $s_f$ based on the extents of a face in world space. However, in cases where an element is strongly deformed, such that one face is much smaller than other faces, this usually indicates strong variations in the FE interpolation, which makes a higher sample density in world space necessary. Therefore, we have decided to choose a constant but sufficiently high $s_f$ before performing curve compression in the next step of our algorithm. This also results in a higher number of proxy rays, which is exploited in the coherency-based curve clustering.

The actual proxy rays are then computed based on the resulting point distribution $P$. To obtain proxy rays from $P$, we consider all pairwise combinations $(p',p'')$, with $p',p'' \in P$. During rendering, a ray intersecting $p'$ and $p''$ would be a line in world space and a curve in $\xi$ space. Figure~\ref{fig:raycoords} shows an example of 9 rays going from one point of the a face to all points of the opposite face. When connecting $p'$ and $p''$ with a straight line, representing a viewing ray in world space, the proxy ray in $\xi$ space can be obtained by sampling along the line in world space and transforming each sample's position into $\xi$ space. While this sampling is performed with a fixed but sufficiently high sampling rate $s_r$, it should be pointed out that it does not represent or determine the sampling rate used later on during rendering. $s_r$ is only used to obtain the geometry of the proxy rays, the sampling rate used for the compositing during ray-traversal in the rendering step can still be chosen independently.

As the set of proxy rays $R$ has to be dense, its size is obviously a limiting factor. Assuming that our subdivision parameter $s_f$ would result in $n$ distinct points along the $u$ and the $v$ coordinate of an element's face, we would have $n^2$ grid points on each face. Thus, we would have $n^2 \cdot f$ grid points in total, assuming that each element has $f$ faces. This would result in $r_u = (n^2 f)^2$ proxy rays to be precomputed. Though, we have kept $s_f$ constant to allow a less complex implementation of the ray-casting process, we have other possibilities to reduce the size of $R$. To achieve this reduction, we exploit properties of the standard volume rendering integral which is used in the rendering stage:

$$ L(x,\omega_o) = L_0 \cdot e^{-\int_{x_0}^{x}\kappa(x'')dx''} + \int_{x_0}^{x}  \! c(x') \cdot e^{-\int_{x'}^{x}\kappa(x'')dx''} dx'$$

\noindent where $c(x')$ represents the emissive color at sample $x'$ and $e^{-\int_{x_0}^{x}\kappa(x'')dx''}$ represents the exponential falloff between $x_0$ and $x$. When exploiting this rendering integral, the same observations hold as when applying the integral within the context of regular volume rendering. For a ray cast through a medium, the application of this integral results in an exponential falloff, i.\,e., when the ray is cast from one start point $x_0$ to one end point $x_n$ the samples in the regions surrounding $x_0$ will have considerable more influence on the image than those around $x_n$. In extreme cases, where structures with a high degree of opacity are rendered, samples close to $x_n$ do not have any influence at all. To select an optimal proxy ray subset during the precomputation stage, we take this observation into account. Therefore, we can use two different point distribution densities for entry and exit points, which is achieved by replacing $s_f$ with $s_f'$ and $s_f''$. Thus we obtain a point distribution of higher density, representing ray entry points, and a point distribution of lower density, representing exit points. Thus, when considering all entry and exit points of these two distributions in a pairwise manner, we achieve an importance-based ray sampling, where rays are more accurately represented when being close to the entry points. In comparison to the $r_u$ rays computed with the uniform proxy ray distribution, the importance-driven ray reconstruction approach results in $r_i = (n_{f} \cdot f) \cdot (n_{b} \cdot f)$ rays, where $n_{f}$ and $n_{b}$ represent the number of precomputed entry and exit points. This results in a decreased number of proxy rays, where the proxy ray set $R$ is more dense towards the respective end points and thus better complies with the exponential falloff which occurs during ray-casting.

As $s_f$ should be high but independent of the sampling rate used during rendering, it is unfeasible to store the proxy rays based on the precomputed samples. Under the assumption that the proxy rays are well-behaved and at least $C^1$ continuous, we can reduce the memory footprint by storing the proxy rays as parametric curves facilitating a lower number of control points than the original sampling density $s_f$. We will present the ramifications of different number of control points on the error in Section~\ref{sec:results}. A spline is a parametric piecewise polynomial curve, which can be represented by bases for efficent computational use. Due to the requirements regarding interpolation of the control points, local control, numerical stability, and non-complex evaluation, Catmull-Rom splines~\cite{catmull74splines} are a natural choice for this parametric representation. Furthermore, Catmull-Rom splines have built-in $C^{0}$ and $C^{1}$ continuity, thus being continuous in position as well as tangent vector, which leads to a smooth curve. Additionally, Catmul-Rom splines allow us to obtain and interpolate the start and end tangent of the curved rays directly.

\subsection{Curve Compression}\label{subsec:curvecompression}

The curve compression, which is used to obtain a manageable set of proxy rays during rendering, first increases the similarity of the proxy rays by exploiting orientation and scaling invariance, before computing a representative subset using clustering. The increased similarity of the proxy rays allows us to compute a smaller subset of proxy rays while maintaining the same quality of compression.

\noindent \textbf{Curve similarity.} In the previous subsection, we have described how to obtain a densely populated set of proxy ray curves, which are represented as parametric curves in $\xi$ space. As this set is obviously too large to be considered during rendering, we will eliminate proxy rays from within this set, by taking into account the following three observations which increase the similarity. First, a proxy ray does not have a pre-defined entry, exit point order. As the ray traversal and the compositing are performed in the rendering stage, we do not need to distinguish between two proxy rays with swapped entry and exit coordinates, if these are the same. The second helpful observation refers to the orientation of proxy rays in $\xi$ space. As a proxy ray represents only the traversal path, but not the dependent variables accessed during a traversal, we do not need to distinguish between two proxy rays which have the same overall geometry, but are differently oriented in $\xi$ space. As long as we know the entry and the exit point of a proxy ray, we can orient a proxy ray's geometry such that it is correctly oriented in $\xi$ space. This property is also related to the third observation, the arc length invariance, which we also exploit to increase the similarity during curve compression. As soon as we know the coordinates of the entry and the exit points, we can not only adapt the orientation of a proxy ray, but also its scaling. Therefore, we do not have to distinguish between proxy rays, which have the same geometry when adapting the scaling factor along their principle axis.

We exploit the three observations stated above, in order to achieve a higher degree of similarity during the subsequent clustering-based curve compression. To do so, we transform the parametric curves representing the proxy rays into a common coordinate frame, without compromising the uniqueness of each proxy ray curve. As proxy ray geometries are equal when their entry and exit points are equal, and as they comply to orientation and scaling invariance, we can transform all proxy rays into a common coordinate frame, where the entry and the exit point lie on the $z$-axis, such that the proxy ray curve's principal axis aligns with the same axis (see Figure~\ref{fig:proxyrayalignment}). To be further able to define the rotation around this axis, we take into account the first control point $c_i$ that is not collinear with the start and the end point, and transform the proxy ray, such that $c_i$ it lies in the $yz$-plane. If there is no such point, the proxy ray is a straight line so that it can be rotated to coincide with the z axis. After this transformation $M_s$ has been applied, all proxy rays lie in the same coordinate frame as shown in Figure~\ref{fig:proxyrayalignment}, and thus the comparison performed during clustering reduces in a higher degree of similarity. While this transformation into a common coordinate frame reduces the number of rays, it requires us to store the angle of rotation $\theta$ for each proxy ray. This is necessary, as $\theta$ is required to be able to compute ${M_s}^{-1}$, which is needed during rendering to correctly orient the precomputed proxy rays.

It should be noted, that this step does not reduce the number of proxy rays which have to be considered, but merely increases the similarity between each proxy ray. This similarity is exploited in the next step, where we find a representative subset of all proxy rays by clustering. The increased similarity allows us to reduce the number of clusters by retaining the representativity.

\noindent \textbf{Curve clustering.} To reduce the size of $R$, we exploit the inter- and intra-element coherence discussed above. This allows us to perform clustering in order to obtain a representative subset of $R$. As several algorithms have been proposed for clustering curves, we have decided to exploit the approach proposed by Abraham et al.~\cite{abraham03clustering}. It has been originally developed for clustering functional data, that is represented through basis functions, which is a very similar setup as it exists in FE models. As this approach applies the clustering on the curve coefficients instead of the control points, it can be proven that the derived cluster representatives are optimal candidates. As we have previously increased the curve similarity of the set of proxy rays $R$ by transforming them into a common coordinate frame, we can use a lower number of cluster representatives then when using the clustering method. This reduces data size as well as computation time needed for the clustering. We will show how the number of clusters affects the achieved results in Section~\ref{sec:results}.

Abraham et al. state that the comparison metric used during the clustering is the most crucial part, as it directly defines the similarity of two curves~\cite{abraham03clustering}. Inspired by their work, we use the spanned area between two proxy ray curves as similarity metric. To compute the spanned area, we equidistantly sample the splines with a high sampling rate, and generate a triangle strip between the sampling points. The similarity can then be expressed by a Riemann sum of the triangle areas. If we have two proxy ray curves $a$ and $b$ with their sampled points $a_1,\dots,a_n$ and $b_1,\dots,b_n$, where due to the scale and orientation invariance $a_1=b_1$ and $a_n=b_n$ hold, we can calculate the approximate area between the splines. Let $x_iy_j$ denote the vector from $x_i$ to $y_j$, then the similarity of $a$ and $b$ is proportional to:

\begin{eqnarray*}
2\cdot d(x,y) &=& \Vert a_1a_2 \times a_2b_2\Vert + \\
&& \sum_{i=2}^{n-2}\Vert a_ia_{i+1} \times a_ib_i \Vert + \Vert b_ib_{i+1} \times b_{i+1}a_{i+1}\Vert + \\
&& \Vert a_{n-1}a_n \times a_{n-1}b_{n-1}\Vert,
\end{eqnarray*}

\noindent which equals twice the spanned area between $a$ and $b$, as the cross product will not give the area of the triangle itself, but the area of the parallelogram spanned by the two vectors. It is clear that this metric fulfills the metric traits of non-negativity, the identity of indiscernibles and symmetry, since all these properties hold for the area computation as well. Although we could have used the analytical solution between two curves, we wanted to use a metric to non-parametric curves as well. The same metric presented here will be used in the error analysis as well, where no closed solution for the bent ray is available. For the actual clustering, we exploit the k-means clustering algorithm, which has been originally presented by Hartigan~\cite{hartigan75kmeans}. It has the benefit that it is robust even with large high-dimensional data and only needs the number of final clusters and a metric as parameters. Unfortunately, the number of clusters is not known a priori and, in fact, there might be no definite answer, which is the best number in general. To deal with this problem, we have evaluated the impact of different numbers of clusters in Section~\ref{sec:results}. Alternatively, one could apply algorithms for finding an optimal number of clusters, such as for instance k-fold cross-validation, which is often used together with k-means. As output, k-means provides the desired clusters with one representative proxy ray for each of them. Besides the obtained cluster representatives, we need to store for each proxy ray, specified by its entry and exit point, its cluster ID to associate it with the cluster it belongs to, as well as the $z$-axis angle $\theta$, which is used to exploit the orientation invariance.

\begin{figure}[b]
  \centering
    \begin{minipage}{1.5cm} 
      \centering 
      \subfigure{\resizebox{1.5cm}{!}{\includegraphics{figures/coincidence/HeartLowPrecisionPoints.jpg}} }\\
      \subfigure{\resizebox{1.5cm}{!}{\includegraphics{figures/coincidence/HeartLowPrecisionSurface.jpg}} }  
     \end{minipage}
     \begin{minipage}{4.25cm} 
      \subfigure{\resizebox{4.25cm}{!}{\includegraphics{figures/coincidence/Coincidence.pdf}} } 
     \end{minipage}
     \begin{minipage}{2.75cm} 
      \subfigure{{\resizebox{2.75cm}{!}{\includegraphics{figures/coincidence/HeartFine.jpg}} } } 
     \end{minipage}    
  \caption{Two cases of depth coincidence can occur, as illustrated ({\it middle}), when rendering adjacent polygons. Point coincidence occurs at the corners of adjacent elements ({\it top left}), while surface coincidence occurs at the touching element's surfaces ({\it bottom left}). All coincidences can be avoided ({\it right}).}  
  \label{fig:depthcoincidence}
\end{figure}

\begin{figure*}[t]
    \centering
    \subfigure[entry world]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastEntryWorld1}}
 	  \subfigure[entry depth]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastEntryDepth1}}
    \subfigure[entry $\xi$]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastEntryXi1}}
    \subfigure[entry face IDs]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastEntryFace1}}
    \subfigure[entry element IDs]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastElementID1}}\\
    \subfigure[exit world]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastExitWorld1}}
 	  \subfigure[exit depth]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastExitDepth1}}
    \subfigure[exit $\xi$]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastExitXi1}}
    \subfigure[exit face IDs]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastExitFace1}}
    \subfigure[exit element IDs]{\includegraphics[width=0.15\linewidth]{figures/breast_mesh/BreastElementID2}}
    \caption{Depth peeling is used to extract the FE layers. The images show the world coordinates, its depth, the $\xi$ coordinate, the face ID and the element ID ({\it from left to right}), while the first row shows the first depth layer and the second row shows the second depth layer.}
    \label{fig:depthpeeling}
\end{figure*}

\section{Bent Ray-Casting}\label{sec:rendering}
While Section~\ref{sec:preprocessing} gives a comprehensive overview of the preprocessing steps needed to obtain the proxy ray set $R$, the actual rendering process is discussed in greater detail within this section. We perform GPU-based ray-casting by exploiting the data generated during the preprocessing stage, i.\,e., a list storing the associated cluster ID as well as the $z$-axis rotation angle $\theta$ for each proxy ray. To be able to reconstruct the actual proxy ray geometry, we further need to have access to the cluster representatives. As the elements are treated individually, we need a way to be able to associate ray segments with the elements and connect them through compositing. Thus, the rendering stage is divided into three substages. First, element peeling which allows to obtain the entry and exit points for each element. Second, ray marching where a ray in $\xi$ space is reconstructed from the preprocessed data before it is traversed. And third, ray interpolation which is used to be able to deal also with less dense proxy ray sets $R$. These three substages are discussed in the following three subsections.


\subsection{Element Peeling}\label{subsec:peeling}

As the FE models visualized consist of several elements, these overlap in image space. Therefore, mechanisms are needed to determine which sequence of proxy rays best represents a specific pixel. To obtain this sequence of proxy rays, we exploit a modified depth peeling approach~\cite{mammen89DepthPeeling}. With this approach, we can peel away layer-by-layer from the rendered FE model by using an additional depth comparison. However, when using depth peeling, it is crucial to be able to distinguish between different fragments solely based on their depth values. In order to deal with problems arising from fragments coinciding at the same depth value, often a user-defined bias parameter is introduced~\cite{Everitt01interactiveorder-independent}. As this coincidence of depth values is not the regular case and can be considered as rather rare compared to those fragments not coinciding in depth, the error introduced due to this bias is often negligible, which makes it a common practice in many computer graphics applications. Unfortunately, in our case the situation is more severe. As a FE model exists of several adjacent, i.\,e., touching, elements, coinciding depth values are the norm and need to be handled robustly. As illustrated in Figure~\ref{fig:depthcoincidence}, two cases of depth coincidence occur. Point coincidences occur at the corners of adjacent elements~({\it top left}) and surface coincidences occur where two adjacent element's surfaces are touching~({\it bottom left}). Since semi-transparency is accumulated along the viewing rays, errors introduced by the bias would be prominent in the rendered image. Therefore, we have realized a multi-layer depth peeling, where we exploit information available in the form of several attributes to resolve the depth coincidences without introducing an error. Figure~\ref{fig:depthpeeling} shows the employed information next to the color coded $\xi$ and world space coordinates of the first two layers of a FE model. Besides the actual depth values, we take also into account the current element's ID, as well as the ID of the current surface in our peeling approach. Thus, we can resolve the described coincidence cases. In general we set the additional depth test to accept all fragments which have a greater or equal depth than the current fragment. To resolve the two coincidence cases, we ensure that each pair of entry- and exit-points has the same element ID. This together with ensuring that they have different face IDs, is enough to correctly resolve the point and the surface coincidence (see Figure~\ref{fig:depthcoincidence}). This additional information also leads to a more stable algorithm w.r.t. the available accuracy of the depth buffer. Using a 16 bit depth buffer compared to a 32 bit depth buffer resulted in the exact same visual result. The approach also works for the situation that the entry and exit point lie on the same surface, as these points are guaranteed to have unequal depth values, thereby making the test for face inequality unnecessary in this specific case.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/curvetexmap-memorylayout}
    \caption{Memory layout used to gain access to the precomputed proxy ray information. To retrieve the information required during rendering in constant time, a hierarchical subdivision is exploited. For each proxy ray, we store the cluster ID as well as the rotation angle $\theta$.}
    \label{fig:memorylayout}
\end{figure}

As we also take into account the current face and element ID during depth peeling, we can use this information to obtain the precomputed values for each proxy ray. We know the FE $c$ each proxy ray segment is traveling through, the face $f_s$ through which it enters $c$ and the face $f_e$ through which it exits $c$. Having this information, we need to identify the precomputed values for the rays closest to the actual ray start point $s_{w_0}$ and its end point $s_{w_n}$. To achieve this in constant time, we exploit the memory layout illustrated in Figure~\ref{fig:memorylayout}. On the highest level of the hierarchy, we arrange the data based on $f_s$ along the $x$-axis and $c$ along the $y$-axis. Thus, we obtain $n_{cells} \cdot n_{faces}$ cells which contain the precomputed information for rays entering a FE $c$ through a specific face $f_s$. Each of these cells is partitioned based on the $f_e$ along the $x$-axis, such that the resulting cells contain only the information for those rays exiting through a specific face $f_e$. Now, we can identify such a cell based on $c$, $f_s$ and $f_e$. To be able to access the precomputed ray data for a ray entering $c$ through $f_s$ at $s_{w_0}$ and leaving through $f_e$ at $s_{w_n}$, we again partition the cells based on the number $n_{entry}$ of precomputed entry points per face. Finally the obtained cells are partitioned based on the number $n_{exit}$ of precomputed exit points per face. To access the data for a ray within this hierarchical memory layout, we require the patch coordinates of $s_{w_0}$ and $s_{w_n}$ in addition to $c$, $f_s$ and $f_e$. Thus, we can obtain the preprocessed proxy ray information, which is stored in texture memory on the GPU, by accessing the hierarchical data structure at position $p$:

\begin{eqnarray*}
p_x & = &(f_s \cdot (n_{entry} \cdot n_{exit} \cdot n_{cells})) + \\
	&  &(f_e \cdot (n_{entry} \cdot n_{exit})) + (s_{w_0,x} \cdot n_{exit}) + s_{w_n,x}\\
p_y & = &(c   \cdot (n_{entry} \cdot n_{exit})) + (s_{w_0,y} \cdot n_{exit}) + s_{w_n,y}
\end{eqnarray*}

\subsection{Ray Marching}

\begin{figure}[b]
\centering
\subfigure[equidistant sampling]{\includegraphics[width=0.45\linewidth, height=0.3\linewidth]{figures/arclength} \label{fig:arclength}}
\subfigure[overshooting]{\includegraphics[width=0.45\linewidth, height=0.3\linewidth]{figures/overshoot} \label{fig:overshoot}}
\protect\caption{\subref{fig:arclength} To achieve equidistant sampling along a curve ray in $\xi$ space ({\it top}), an arc length parametrization is applied, resulting in a non-uniform sampling in world space ({\it bottom}). \subref{fig:overshoot} Rays penetrating multiple elements must be processed, such that constant sampling step sizes are obtained at the borders. The overshoot $e$ of $FE_0$ can be used to offset the first sampling point in element $FE_1$.}
\label{fig:arclengthplusovershoot}
\end{figure}

%\begin{figure}[b]
%\centering
%\includegraphics[width=0.5\linewidth]{figures/arclength}
%\caption{To achieve equidistant sampling along a curve ray in $\xi$ space ({\it top}), an arc length parametrization is applied, resulting in a non-uniform sampling in world space ({\it bottom}).}
%\label{fig:arclength}
%\end{figure}

%\begin{figure}[t]
 %   \centering
%    \includegraphics[width=0.3\linewidth]{figures/overshoot}
%    \caption{Rays penetrating multiple elements must be processed, such that constant sampling step sizes are obtained at the borders. The overshoot $e$ of $FE_0$ can be used to offset the first sampling point in element $FE_1$.}
%    \label{fig:overshoot}
%\end{figure}

\begin{figure}[t]
    \centering
    \subfigure[$2 \times 2$ precomputed proxy rays per element]{\includegraphics[width=0.49\linewidth]{figures/heart-2-new-with-closeup.pdf}
    \includegraphics[width=0.49\linewidth]{figures/heart-2-rr-ii-new-with-closeup.pdf}}\\
    \subfigure[$10 \times 10$ precomputed proxy rays per element]{\includegraphics[width=0.49\linewidth]{figures/heart-10-new-with-closeup.pdf}
    \includegraphics[width=0.49\linewidth]{figures/heart-10-rr-ii-new-with-closeup.pdf}}
    \caption{Application of our approach with different precomputation and rendering parameters. In the two rows, $2 \times 2$ ({\it a}) and $10 \times 10$ ({\it b}) proxy rays have been precomputed per element. Within each row, we show the application of standard ray interpolation ({\it left}), as well as intra- and inter-ray interpolation ({\it right}). As shown in the close-ups, intra- and inter-ray interpolation reduces noticeable artifacts. Artifacts are still noticeable with $2 \times 2$ proxy rays, as shown in ({a, \it right}), while $10 \times 10$ proxy rays has no noticeable artifacts ({b, \it right}).}
    \label{fig:rayinterpolation}
\end{figure}

Once we know which proxy ray information needs to be accessed in the hierarchy, we can reconstruct its ray geometry by retrieving the spline coefficients for the obtained cluster ID. The reconstructed spline is then transformed back from the canonical orientation along the $z$-axis by applying ${M_s}^{-1}$, so the spline lies between the entry- and the exit-point of the actual viewing ray in world coordinates. Although the reconstructed spline is similar for pixels in a local neighborhood, the specific path is different, because it is fitted to the specific entry and exit-points of the viewing ray. Once the proxy ray curve has been transformed back into its originally location, we can march along it and perform the actual bent ray-casting by fetching the dependent variables from the $\xi$ space using the spline representation. The expensive world-to-material transformation that would be necessary here in the straightforward approach is replaced by an evaluation of the precomputed spline representation. The viewing ray is sampled equidistantly in world space to comply with the rendering integral.   %However, we need to ensure that we have an equidistant sampling along the reconstructed ray in order to obtain optimal results. The splines, however, are parameterized by the parameter value $t \in [0,1]$ in world coordinates, which does not take the shape of the ray in $\xi$ space into account, e.\,g. given a spline $s$, $\Vert s(i) - s(j) \Vert \neq \Vert i - j \Vert$. That means that an equidistant sampling in world space will result in sampling points that are not equidistant along the curve in $\xi$ space. As illustrated in Figure~\ref{fig:arclength}, we address this problem by using an arc length parametrization as presented by Guenter and Parent~\cite{guenter90arclength}. This leads to a function $t \mapsto t'$. If the sampling is done using $t'$ instead of $t$, the sampled values will be equidistant along the curve in $\xi$ space.

The subdivision of a curve ray penetrating the whole FE model into segments based on the penetrated FEs, requires an appropriate border handling scheme. If we sample the ray equidistantly in a na\"ive way, the last point on the ray, which lies on the exit face of a FE, might not coincide with an intended sampling point (see Figure~\ref{fig:arclength} (right)). Special consideration is for instance needed when handling a ray $a$ which is divided into two segments $r$ and $s$, in such a way that $r_n = s_0$, i.\,e., the last sample point of $r$ is the first sample point of $s$. In this situation, we would have an equidistant sampling rate of $\left|a_i - a_{i+1}\right| = \delta$ for the two rays. However, the na\"ive implementation will have $\left|r_{n-1} - r_{n}\right| \neq \delta$. To remedy this, we do not sample the last point on each ray but save the overshoot instead and use it as an offset for the first sampling point in the next ray segment.

\subsection{Ray Interpolation}

In order to further improve the image quality of our ray-casting approach, we introduce two specialized interpolation schemes: inter-ray interpolation and intra-ray interpolation. With the inter-ray interpolation, we use the four nearest proxy rays  instead of the proxy ray for a given pixel position, and interpolate between them. This interpolation is carried out by using bilinear interpolation at every sample position along the ray. The interpolation factors for these bilinear interpolations continuously vary along the ray, based on the factors derived for the entry and the exit face. The intra-ray interpolation instead, employs swapping of entry and exit points in order to obtain a mirrored copy of the current ray. Thus, we can interpolate between the original ray at position $t$ and the mirrored ray at position $t'=1-t$ while traversing the initially obtained ray. Both of these interpolation schemes improve the image quality in situations where only a low resolution is available, as we show in Figure~\ref{fig:rayinterpolation}.

\begin{figure}
    \centering 
    \subfigure[surface rendering]{\includegraphics[width=0.45\linewidth]{figures/fem-breast}}
    \subfigure[volumetric rendering]{\includegraphics[width=0.45\linewidth]{figures/Breast}}
    \caption{FE breast model with MRI material space rendered as surface representation showing the deformed and the undeformed surface (a), and showing the deformed volume with our interactive volumetric approach (b).}
    \label{fig:breast}
\end{figure}

\begin{figure}
    \centering 
    \subfigure[polygonal rendering]{\includegraphics[width=0.45\linewidth]{figures/fem-tongue}}
    \subfigure[opaque volumetric rendering]{\includegraphics[width=0.45\linewidth]{figures/tongue3}}\\
    \subfigure[translucent volumetric rendering]{\includegraphics[width=0.90\linewidth]{figures/fem-tongue-new-sidebyside}}
    \caption{FE tongue model rendered as a polygonal representation (a), with our interactive volumetric approach~(b,c).}
    \label{fig:tongue}
\end{figure}

\section{Results}\label{sec:results}

To assess the quality and the performance of the proposed visualization technique, we have applied it to different data sets. Below, we will discuss the visual results, the introduced errors as well as the performance of our approach.

\subsection{Visual Results}

To demonstrate the visual outcome of the proposed concepts, we have applied them to different FE models. These vary with respect to the number of cells and the order of the used interpolation functions. Although we include only functions of order 2 or 3, our method is not limited to any specific order for as long as the coherencies are fulfilled. In the following paragraphs we describe these models and the visual results achieved.

\noindent \textbf{Breast model.} As the most simple tested model with respect to number of elements and material parameters, we have applied our approach to a deformation model of a breast data set, consisting of 60 tricubic elements, acquired from an MRI scan. The dependent variables in this case are the actual intensities of the MRI field, and the FE modeling has been applied to predict the supine shape of the data set originally acquired in the prone position. Figure~\ref{fig:breast} shows a comparison of the supine position rendered with our interactive volumetric approach (b) as compared to a standard surface based representation (a).

\noindent \textbf{Tongue model.} Furthermore, we have applied our approach to a model of a human tongue, consisting of 64 tricubic elements, with simulated tissue densities and muscle fiber directions. In this case, each node stores one coordinate vector and seven derivatives and mixed derivatives for the tensor product of the cubic Hermite interpolation functions in all three coordinate directions. The results as compared to a polygonal representation are shown in Figure~\ref{fig:tongue}. Especially in this case, the multi-parametric capabilities of our algorithm are beneficial, as the tongue model contains a relatively high number of parameters. When one would apply resampling to all of these parameter volumes instead, this would result in an unmanageable amount of data for a reasonable resolution.

\noindent \textbf{Heart model.} A heart data set, which consists of 16 bicubic-linear elements (i.e.~two interpolation functions are of cubic order, the other one is linear),  where the strain values for each point have been computed from displacements  obtained from tagged MRI data, serves as the third example (see Figure~\ref{fig:teaser}). Since the computation of the strain values was performed over the FE geometry, but using a numerical optimization technique, the strain values are represented as $11 \times 11 \times 6$ regular grids in material space, which are trilinearly interpolated. For our visualization, we have taken into account the three principal strain directions, which are the eigenvalues of the tensor. The principle strains include the maximum and minimum deformation and the corresponding eigenvectors give the direction of these deformations. These directions are circumferential, longitudinal and radial.

\begin{table}[b]
  \caption{Performance analysis (in fps) of our technique for different grid sizes compared to a straightforward (SF) implementation. The setups match the first three subfigures in Figure \ref{fig:teaser}, which are images showing radial strain, circumferential strain, and longitudinal strain.}
  \label{tab:performance}
  \begin{center}
    \begin{tabular}{|l|c|c|c|c|c|}
      \hline
      Figure \ref{fig:teaser} & Pixel & GPU & \multicolumn{3}{c|}{Our Technique [Grid, Clusters]}\\
      TF Setups & Res & (SF) & $3^2,1024$ & $5^2,2048$ & $10^2,4096$ \\
      \hline
      Radial & $256^2$ & 2.58 & 14.52 & 14.25 & 14.05\\
      Strain & $512^2$ & 1.33 & 11.40 & 11.71 & 11.02\\
      & $1024^2$ & 0.41 & 7.09 & 7.02 & 6.83\\
      \hline
      Circum- & $256^2$ & 2.60 & 14.63 & 14.81 & 14.21\\
      ferential & $512^2$ & 1.30 & 12.09 & 11.85 & 11.72\\
      Strain & $1024^2$ & 0.40 & 7.15 & 7.12 & 7.10\\
      \hline
      Longi- & $256^2$ & 2.53 & 14.66 & 14.70 & 14.74\\
      tudinal & $512^2$ & 1.35 & 11.78 & 11.78 & 11.83\\
      Strain & $1024^2$ & 0.40 & 7.12 & 7.12 & 6.88\\
      \hline
    \end{tabular}
  \end{center}
\end{table}

\begin{figure}
    \centering
    \subfigure[Straightforward approach]{\includegraphics[width=0.32\linewidth]{figures/heart-comparison-straight-forward.png}}
    \subfigure[Our approach]{\includegraphics[width=0.32\linewidth]{figures/heart-comparison-clustering.png}}
    \subfigure[Visual difference (emphasized 10 times)]{\includegraphics[width=0.32\linewidth]{figures/heart-comparison-diff-times-ten.png}}
    \caption{Visual error as obtained when applying our technique. A straightforward volumetric approach (a) and our interactive volumetric approach (b) have been compared, which results in the shown 10 times exaggerated error image (c). Without this exaggeration, the error would be hardly noticeable.}
    \label{fig:heart-analysis}
\end{figure}

\subsection{Error Analysis}\label{subsec:error}
\begin{figure*}[t]
    \centering
    \subfigure[~]{\includegraphics[width=0.32\linewidth]{measurements/spline_approximation/spline_approximation.pdf} \label{fig:spline_control_points_error}}
    \subfigure[~]{\includegraphics[width=0.32\linewidth]{measurements/clustering/clustering_error.pdf} \label{fig:clustering_epsilon_error}}
    \subfigure[~]{\includegraphics[width=0.32\linewidth]{measurements/cluster_measurements/clusterErrorPlot.pdf} \label{fig:clustering_visual_error}}

    \caption{\textbf{Error measurements.} \subref{fig:spline_control_points_error} shows the error introduced by the approximation of the correct bent ray by sampling a spline. The abscissa shows the number of control points used to construct the spline, where 2 points represent a straight line. \subref{fig:clustering_epsilon_error} shows the error for varying numbers of clusters depending on the number of iterations after which the k-means algorithm has stopped. Based on the heart data set with a grid resolution of $5\times5$, which resulted in 360.000 rays. \subref{fig:clustering_visual_error} shows the normalized absolute pixel error (background pixels are neglected) plotted against the percentage of clusters relative to the total amount of curves. The calculations have been performed on the subfigures of Figure~\ref{fig:teaser}, thus varying the transfer function and strain direction. All error values were normalized w.r.t the highest error.}
    \label{fig:error}
\end{figure*}

%\begin{figure*}[t]
%    \centering
%    \subfigure[The error introduced by the approximation of the correct bent ray by sampling a spline. The abscissa shows the number of control points used to construct the spline, where 2 points represent a straight line.]{\includegraphics[width=0.3\linewidth]{measurements/spline_approximation/spline_approximation.pdf}}
%    \hspace*{\hfill}
%    \subfigure[The error for varying numbers of clusters depending on the number of iterations after which the k-means algorithm has stopped. Based on the heart data set with a grid resolution of $5\times5$, which resulted in 360.000 rays.]{\includegraphics[width=0.3\linewidth]{measurements/clustering/clustering_error.pdf}}
%    \hspace*{\hfill}
%    \subfigure[The normalized absolute pixel error (background pixels are neglected) plotted against the percentage of clusters relative to the total amount of curves. The calculations have been performed on the subfigures of Figure~\ref{fig:teaser}, thus varying the transfer function and strain direction.]{\includegraphics[width=0.3\linewidth]{measurements/cluster_measurements/clusterErrorPlot.pdf}}

%    \caption{\textbf{Error measurements.}All error values were normalized w.r.t the highest error.}
%    \label{fig:error}
%\end{figure*}

The presented rendering approach contains several steps which potentially introduce visual errors. First, the proxy ray generation explained in Subsection~\ref{subsec:proxyraygeneration} introduces an error based on the number of actual proxy rays. The visual impact of this error is analyzed together with the potential clustering error in Figure~\ref{fig:rayinterpolation}. In the two rows, different numbers of proxy rays have been computed. In the top row, a very low number with $2 \times 2$ proxy rays per element face has been chosen. The bottom row uses $10 \times 10$ proxy rays instead. As it can be seen, the error introduced by a low number of proxy rays is clearly visible. However, when using $10 \times 10$ proxy rays, no artifacts are visible when using the proposed ray interpolation mechanism. As the clustering itself also introduces an error, we have analyzed the impact in Figure~\ref{fig:clustering_epsilon_error}, which shows the error for a varying number of clusters. It is obvious that the error becomes smaller if we use more clusters. The number of clusters can, depending on data set, be quite low in relation to the number of splines, as seen in Figure~\ref{fig:clustering_visual_error}. This is due to the fact that clustering of the curves is performed without consideration for the angle, as this stored in separately, which increase the correlation between the curves. While these two analyses show the error for the proxy ray computation and the clustering separately, we have also compared the results achieved by our algorithm to the ground truth, a straightforward raycaster as described in Section~\ref{sec:preprocessing}, which evaluates the world-to-material space transformation during each sample step. We chose this method as a comparison, because it produces the smallest error by itself and is thereby suited to demonstrate the error introduced by our method. As it can be seen in Figure~\ref{fig:heart-analysis}, the visual error is rather low, as only a ten times exaggeration of the generated error image makes it noticeable.

We described in Section~\ref{subsec:proxyraygeneration} that we can use a varying number of control points to represent the proxy ray by a spline. This, of course, is an approximation of the original bent ray we get by using the straightforward implementation. We want to analyze the effect that this parameter has on the error introduced by the representation. Figure~\ref{fig:spline_control_points_error} shows this error which was computed by comparing the spline representation to the ray where each sample point is transformed. We sampled both rays at a sufficiently high rate of 1024 points and computed the area metric as described in Section~\ref{subsec:curvecompression}. The abscissa shows the number of control points which comprised the spline, where 2 points let to a straight line. For easier comparison, all error values were normalized with regard this value. The error is of course highly dependent on the shape of the FE in the model and we computed the error for all proxy rays in the \emph{Heart} model described above.

Another source of error which is introduced into the pipeline is the tessellation of the curved elements. In order to perform the depth peeling, we need to represent the curved surfaces by piecewise-linear approximations. Although this results in an error on the final image, neither the speed nor the memory footprint of our method is affected by the tessellation of the curved elements.

\subsection{Performance Analysis}\label{subsec:performance}

\begin{table}[b]
  \caption{Memory consumption of data structures used during rendering for different grid resolutions and with a variation of number of clusters.}
  \label{tab:data_size}
  \begin{center}
    \begin{tabular}{|l|c|c|c|c|c|c|}
      \hline
      Entry & Exit &  Without & \multicolumn{3}{c|}{With Certain Number of Clusters}\\
      Points & Points & Clustering & 512 & 2048 & 4096\\
      \hline
      3 & 3 & 12.8\,MB & 1.9\,MB & 2.3\,MB & 2.8\,MB \\
      7 & 5 & 200.4\,MB & 27.3\,MB & 28.2\,MB & 29.0\,MB\\
      8 & 8 & 672.8\,MB & 92.2\,MB & 94.1\,MB & 95.2\,MB \\
      10 & 10 & 1.65\,GB & 229.5\,MB & 232.2\,MB & 234.4\,MB \\
      \hline
    \end{tabular}
  \end{center}
\end{table}

%\begin{figure}[t]
%    \centering 
%		\includegraphics[width=1.0\linewidth]{measurements/spline_approximation/spline_approximation.pdf}
%    \caption{The error introduced by the approximation of the correct bent ray by sampling a spline. The abscissa shows the number of control points used to construct the spline, where 2 points represent a straight line. The error values were normalized w.r.t. this value.}
%    \label{fig:spline_control_points_error}
%\end{figure}
%
%\begin{figure}[t]
%    \centering 
%		\includegraphics[width=1.0\linewidth]{measurements/cluster_measurements/clusterErrorPlot.pdf}
%    \caption{The normalized absolute pixel error (background pixels are neglected) plotted against the percentage of clusters relative to the total amount of curves. The calculations have been performed on the subfigures of Figure~\ref{fig:teaser}, thus varying the transfer function and strain direction.}
%    \label{fig:clustering_visual_error}
%\end{figure}
%
%\begin{figure}[t]
%    \centering 
%    \includegraphics[width=0.85\linewidth]{measurements/clustering/clustering_error.pdf}
%    \caption{The error for varying numbers of clusters depending on the number of iterations after which the k-means algorithm has stopped. Based on the heart data set with a grid resolution of $5\times5$, which resulted in 360.000 rays. Initial error for 32 clusters has the value 1.}
%    \label{fig:clustering_epsilon_error}
%\end{figure}

We have to differentiate between the precomputation and the rendering step in the performance analysis. Although, the time used for precomputation is dependent on both the proxy ray computation and the clustering, the latter takes more time than the former. In our unoptimized CPU implementation, the clustering for a $5\times5$ proxy ray setup with $8192$ clusters takes roughly 4-6 hours, while a $15\times15$ setup with $8192$ clusters takes approximately 72-84 hours of time. We managed to reduce this time with an unoptimized OpenCL implementation by an order of magnitude, which is in line with former works~\cite{mess10gpuclustering}. The clustering and all performance tests were executed on an Intel Xeon Quad-Core 3.06\,GHz with 6\,GB of working memory, equipped with a Geforce~580 card with 1\,GB of memory.

Table~\ref{tab:performance} shows the rendering performance of the presented volumetric FE visualization technique, as compared to the straightforward volumetric GPU implementation. As can be seen, the performance of our technique is affected by image resolution and independent of the amount of clusters and proxy rays. The rendering quality is thus affected by a trade-off between the number of proxy rays used during the precomputation and the number of clusters used during the curve compression. In the worst case tested, our technique provides above $7$\,fps instead of $0.4$\,fps. While the $14.81$\,fps achieved with our method in the best tested case enabled full interactivity, which for instance allows for changing the parameter mapping or the camera orientation, the $7$\,fps also resulted in an interactive experience.
%Increasing the amount of proxy rays result in a significantly larger amount of memory needed during rendering, as shown in Table~\ref{tab:data_size}. However, increasing the amount of clusters result in a rather low increase in data size, as angles and cluster IDs are stored for all rays independent of the number of clusters. Table~\ref{tab:performance} further shows that we have a performance gain of more then five times as compared to the straightforward approach.

\subsection{Memory requirements}\label{subsec:memory}
A limiting factor of the presented method is the memory that is required to store the proxy rays, angles and cluster IDs. Besides the computational time, the available graphics memory is the only limitation to the number of elements, grid resolution, and number of clusters. In this section, we will show the impact of each of these factors on the memory requirements. $e$ will denote the total number of elements, $f$ is the number of faces per element, $n$ is the grid paramenter, so that $n^2$ is the number of points per face, $s_r$ the number of control points per ray, and $c$ the number of clusters. Following Section~\ref{subsec:proxyraygeneration}, the total number of proxy rays is $r_u = (n^2f)^2$. This will result in $p = e \cdot r_u \cdot s_r$ points which have to be transformed and stored. It should be noted that this data does not need to fit on the graphics card and is only an intermediate result. The required memory after the clustering can be estimated by: $c \cdot s_r + e \cdot r_u \cdot 2$. The first part is the storage necessary for the clustered control points of the splines. The second part is the lookup table as described in Section~\ref{subsec:peeling} and needs to store both the cluster ID as well as the angle. Exemplary values for the heart are given in Table~\ref{tab:data_size}. 

\section{Conclusions \& Future Work}\label{sec:conclusions}
In this paper we have presented a ray-casting based approach which allows the interactive exploration of high-order FE models. By shifting the computationally complex world to $\xi$ space transformation from the rendering stage into a preprocessing stage, we are able to achieve interactive frame rates and thus allow for an interactive in-detail inspection of FE models on standard GPUs. Thus we are able to change transfer functions and other parameters, which is essential for interactive multi-parametric data exploration. We have demonstrated the outcome of the presented approach by applying it to several real-world data sets from different disciplines, and by analyzing the errors introduced by our approach. Furthermore, we have discussed the rendering performance gain as well as the memory requirements.

Even though the examples used in the results section are of cubic order, we have made no assumptions on the complexity of the basis functions. This means, that as long as the inter- and intra-element coherency is fulfilled, we do not have a limit w.r.t. the model.

In the future, there are several opportunities to further improve the presented concepts. When precomputing the proxy rays, we currently use a equidistant layout for the generation of the set $P$ of uniformly distributed start and end points. In comparison to such a equidistant layout, more elaborate sampling schemes, as used in many areas of computer graphics, might lead to an improved quality with the same number of proxy rays. While, our approach currently supports scalar data only, in the future we plan to use the proposed method to visualize vector and tensor data defined over $\xi$ space.



\todo{Discuss how to extend to other element shapes.}



\acknowledgments{We thank all reviewers for their fruitful comments which
helped to greatly improve this paper. We would also like to thank Assoc.-Prof. Alistair Young from the Department of Anatomy and Radiology from the University of Auckland, New Zealand, who kindly provided the tagged MRI data of a healthy left ventricle. This work was partly supported by grants from the Swedish Research Council (VR, grant 2011-4113), the Excellence Center at Link\"oping and Lund in Information Technology (ELLIIT) and the Swedish e-Science Research Centre (SERC). The presented concepts have been realized using the Voreen open source visualization framework (www.voreen.org).}

\newpage

\bibliographystyle{abbrv}
\bibliography{literature-fem}

\end{document}