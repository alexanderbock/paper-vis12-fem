- Revise Table 2 (use correct results instead of textfile size)

Summary Rating
==============

     7.		Concern was also expressed about the level of
     accuracy of the images, and about the evaluation of their
     quality.




Primary Reviewer
================

   I also note that the memory footprint seems to be
   much larger than that of the primary data - and this is likely to be a
   major constraint when applying the technique to other problems.



Secondary Reviewer
==================

   The authors do not provide sufficient details to help make the work
   reproducible.  Timing results are given, but full articulation of the
   hardware used, compilers, etc., are not given.  In addition, there is not
   enough implementation details specified that would help a reader (or
   graduate student) to replicate and then build on this work without
   significant effort.

   The authors also make two statements about high-order FEM that are
   suspicious:

   - The authors in Section 4 say that "resolution needs to be high enough
   that variations of the unknown variable can be adequately approximated by
   the underlying FE basis functions".  This is indeed the "goal", but is
   not always realized in a real engineering simulation.  Being moderately
   resolved or under-resolved is more often the truth as people push the
   boundaries in terms of what is computable.  When things are only
   marginally resolved, how does the system/method you propose handle such
   things?  In theory, one could increase the number of proxy rays that one
   generates to help guarantee that the error is sufficiently low.  However,
   in doing so one would "sacrifice" interactivity.

   - There is a statement that most elements are "well-behaved".  This is
   again the goal, but one thing that practitioners of high-order FEM often
   quote (from proofs by Babuska) is that high-order methods are LESS
   sensitive (in terms of convergence) to ill-formed elements than
   traditional low-order elements.  Hence, the likelihood that you will have
   a poor-quality element in a realistic high-order FEM mesh is greater as
   many practitioners take advantage of this (trying to get the largest
   elements possible so that they can increase the polynomial order for
   efficiency).  How would the proposed system/methodology handle such
   elements?  All the examples presented are for the "very" nice case -- not
   the realistic engineering cases.

   For a high-order paper, there are no references to texts on high-order
   methods that would help the readership.   Szabo's book, Chris Schwab's
   book, Paul Fisher's book, Karniadakis' book ... all books that help lay
   some of the context for the math and the practical engineering.


Reviewer 1
==========

   I had a difficult time understanding how proxy rays are generated and
   used.  Better figures could help alleviate this.  I also have little clue
   in how all of these proxy rays are stored (Figure 8 did not help me).

   I was unable to watch half of the video.  There were extreme encoding
   issues that made the video render as a garbled mess.


Reviewer 3
==========

   For Figures 11, 14, and 15 to be meaningful, a description of the
   baseline image used must be provided.  From Figures 11 and 14, it appears
   accuracy was assumed once no obvious visual artifacts were seen, which is
   not a compelling argument for accuracy.

   The scalability of this algorithm is concerning.  In Section 3, it is
   claimed that optimal results require a moderate number of elements, yet
   in many areas (computational fluid dynamics, for example), high-order
   fields can require tens and hundreds of thousands of elements to achieve
   the required level of accuracy.  The performance numbers provided
   indicates days of processing for very small models, which will severely
   limit the general applicability of the algorithm.  

   Section 4.1, 4th paragraph.  The description of this paragraph is at odds
   with the volume rendering equation being used on page 4.  In this form,
   areas of the volume can emit without incurring any occlusion, which means
   that, depending on the transfer function, the areas near the entrance and
   exit of the element can contribute equally.

   Better accuracy results.  There are many areas where approximations are
   introduced, and it isn't clear how these tradeoffs affect the final
   image.  It would be nice to see the impact on execution time and final
   image of all of the free parameters, not just the number of poxy rays.

   Discussion of scalability.  This algorithm has significant execution
   times in the pre-processing step for even small models, and also uses a
   significant amount of memory.  It does not appear that this algorithm
   will scale well.  

Reviewer 5
==========

   In my view the presentation is very "wordy" and shows to little math in
   order to be easily understood. In some paragraphs the text is not easily
   accessible and I would rather prefer the authors to write down
   explanations in mathematical language (formulas, equations) using
   appropriate notation rather than explaining things in words. 

   In this spirit I would appreciate to have a clear definition and notation
   for the geometric elements being worked with, the transformation between
   world and material space, the curves and their parameterizations, etc.
   
   Also in Section 4.1 the authors claim that they can use two different
   point distributions for entry and exit points of the proxy rays, because
   the sample points close to the exit point will have only a small
   contribution due to the exponential falloff. However, I would think that
   this also depends on the distance between entry and exit point and the
   shape of the kernel K. The authors should comment on that.
   